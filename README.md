
# TizaMcpRag

Projeto que integra execu√ß√£o de MCPs (procedures) e consulta RAG (documentos) com IA local via Ollama.

---

# üî• Fluxograma de Comunica√ß√£o

![Fluxograma TizaMcpRag](./fluxograma_tizamcprag.png)

---

## Servi√ßos Dispon√≠veis

- **n8n**: Orquestra√ß√£o de fluxos
- **api_rag**: Consulta a documentos (RAG)
- **api_mcp**: Execu√ß√£o de procedures SQL
- **ollama**: Servidor de LLMs local

## Pr√©-requisitos

### Suporte a GPU (Recomendado)

Para melhor performance dos modelos, √© recomendado usar GPU. Instale:

1. [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
2. Driver NVIDIA atualizado

### Configura√ß√£o do Banco de Dados

1. Copie o arquivo de exemplo de configura√ß√£o:
```bash
cp .env.example .env
```

2. Edite o arquivo `.env` com as credenciais do seu banco SQL Server:
```env
DB_SERVER=seu_servidor
DB_DATABASE=seu_banco
DB_USERNAME=seu_usuario
DB_PASSWORD=sua_senha
```

3. Execute o script de cria√ß√£o das procedures:
```bash
# Use seu cliente SQL Server favorito para executar:
sql_server/scripts/create_procedures.sql
```

## Como rodar o projeto

```bash
docker-compose up -d
```

Ap√≥s iniciar os containers, aguarde alguns minutos para que os modelos do Ollama sejam baixados automaticamente.

## Configura√ß√£o do n8n

### Instala√ß√£o do Node MCP

1. Acesse o n8n em `http://localhost:5678`
2. V√° em Settings > Community Nodes
3. Clique em "Add node by providing npm package name"
4. Insira `n8n-nodes-mcp`
5. Se a instala√ß√£o autom√°tica falhar, siga os passos para instala√ß√£o manual:
   - Copie a pasta `n8n-nodes-mcp` para `.n8n/custom`
   - Reinicie o container do n8n

### Importa√ß√£o do Fluxo

1. No n8n, clique em "Import from File"
2. Selecione o arquivo `scripts/TizaChatBot.json`
3. Ajuste as credenciais e endpoints conforme necess√°rio

## Atualizando documentos RAG

Adicionar documentos em `api_rag/documentos/` e executar:

```bash
docker-compose exec tizamcprag-api-rag python preprocess_docs.py
```

## Modelos do Ollama

O sistema requer os seguintes modelos do Ollama:
- `nomic-embed-text`: para gera√ß√£o de embeddings dos documentos
- `gemma3:latest`: para gera√ß√£o de respostas da API RAG
- `mistral`: para o agente de chat no n8n (requer suporte a tools)

A instala√ß√£o dos modelos √© feita automaticamente na primeira inicializa√ß√£o do sistema.

### Configura√ß√£o do Agente n8n

Ao importar o fluxo no n8n, configure o n√≥ AI Agent para usar o Ollama:
1. Selecione o n√≥ AI Agent
2. Em "Service", escolha "Ollama"
3. Configure o endpoint: `http://ollama:11434`
4. Selecione o modelo `mistral`

---

## Observa√ß√µes
- Todos os servi√ßos s√£o gerenciados via `docker-compose`.
